{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "58fbf49e-f6a3-4be2-a21f-087946ac0595",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get the file name from the Azure Data Factory\n",
    "fileName = dbutils.widgets.get('fileName')\n",
    "# fileName = 'ProductData.csv'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6259a115-ab8d-46f2-9a36-7bda72962b56",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "from pyspark.sql import types as T\n",
    "#from datetime import datetme as dt\n",
    "\n",
    "storageAccount='iot12colinstorageaccount'\n",
    "storageContainers = [\n",
    "    'input', 'staging', 'rejected'\n",
    "]\n",
    "stgAccountSASTokenKey = 'storage-sas'\n",
    "databricksScopeName ='azurekeyvault'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f3cab102-2917-4857-a1d6-919e35282715",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/mnt/input has been unmounted.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# dbutils.fs.mounts()\n",
    "# dbutils.fs.unmount('/mnt/input')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "411bad35-28a5-480e-a576-7b8895d04379",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mounted /mnt/input successfully\n",
      "Mount point /mnt/staging already mounted\n",
      "Mount point /mnt/rejected already mounted\n"
     ]
    }
   ],
   "source": [
    "sasKey = dbutils.secrets.get(\n",
    "    scope = databricksScopeName, \n",
    "    key= stgAccountSASTokenKey\n",
    ")\n",
    "for container in storageContainers :\n",
    "    mountPoint = '/mnt/{}'.format(container)\n",
    "    if any(mount.mountPoint == mountPoint for mount in dbutils.fs.mounts()):\n",
    "        print('Mount point {} already mounted'.format(mountPoint))\n",
    "        continue\n",
    "\n",
    "    dbutils.fs.mount( \n",
    "        source = 'wasbs://{}@{}.blob.core.windows.net'.format(container, storageAccount), \n",
    "        mount_point= mountPoint, \n",
    "        extra_configs = {'fs.azure.sas.{}.{}.blob.core.windows.net'.format(container,storageAccount) : sasKey}\n",
    "    )\n",
    "    print('Mounted {} successfully'.format(mountPoint))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7d70cd24-81ed-48c4-b434-175ec1a06169",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "df_formats = spark.createDataFrame([\n",
    "    {'FileName': 'Product', 'ColumnName': 'StartDate', 'ColumnDateFormat': 'MM-dd-yyyy'},\n",
    "    {'FileName': 'Product', 'ColumnName': 'EndDate', 'ColumnDateFormat': 'MM/dd/yyyy'},\n",
    "    {'FileName': 'Product', 'ColumnName': 'CreateDate', 'ColumnDateFormat': 'MM/dd/yyyy'},\n",
    "    {'FileName': 'Product', 'ColumnName': 'ModifiedDate', 'ColumnDateFormat': 'MM/dd/yyyy'}\n",
    "])\n",
    "# display(df_formats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_csv_schema(*args):\n",
    "    return T.StructType([\n",
    "        T.StructField(*arg)\n",
    "        for arg in args\n",
    "    ])\n",
    "def read_csv(fname, schema):\n",
    "    return spark.read.csv(\n",
    "        path=fname,\n",
    "        header=True,\n",
    "        schema=get_csv_schema(*schema)\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2c379ded-f058-4714-8d50-20b73c58a6fb",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "debug {}.test [REDACTED]\n"
     ]
    }
   ],
   "source": [
    "errorFlag=False\n",
    "errorMessage = ''\n",
    "# df1 = spark.read.csv('/mnt/input/'+fileName, inferSchema=True, header=True\n",
    "df = read_csv('/mnt/input/'+fileName, schema = [\n",
    "    (\"ProductId\", T.StringType(), False),\n",
    "    (\"Price\", T.DecimalType(10,4), True),\n",
    "    (\"guid\", T.StringType(), True),\n",
    "    (\"StartDate\", T.StringType(), True),\n",
    "    (\"EndDate\", T.StringType(), True),\n",
    "    (\"CreateDate\", T.StringType(), True),\n",
    "    (\"ModifiedDate\", T.StringType(), True)\n",
    "])\n",
    "#display(df1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6628379d-f0f5-418b-9926-ef1584d476b8",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n",
      "500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rule 1\n",
    "\n",
    "totalcount = df.count()\n",
    "print(totalcount)\n",
    "distinctCount = df.distinct().count()\n",
    "print(distinctCount)\n",
    "if distinctCount !=totalcount:\n",
    "    errorFlag = True\n",
    "    errorMessage = 'Duplication Found. Rule 1 Failed'\n",
    "print(errorMessage)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "638c358e-b149-4675-ad1c-33376b3d542e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StartDate yyyy-dd-MM\n",
      "All rows are good for  StartDate\n",
      "EndDate yyyy/dd/MM\n",
      "All rows are good for  EndDate\n",
      "CreateDate yyyy-dd-MM\n",
      "All rows are good for  CreateDate\n",
      "ModifiedDate yyyy-dd-MM\n",
      "All rows are good for  ModifiedDate\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Rule 2\n",
    "for r in df_formats.collect():\n",
    "    colName = r['ColumnName']\n",
    "    colFormat =r['ColumnDateFormat']\n",
    "    print(colName, colFormat)\n",
    "    #display(df1.filter(F.to_date(colName, colFormat).isNull() ==True))\n",
    "    formatCount = df.filter(F.to_date(colName, colFormat).isNotNull() ==True).count()\n",
    "    if formatCount != totalcount:\n",
    "        errorFlag = True\n",
    "        errorMessage = errorMessage +' DateFormat is incorrect for {} '.format(colName)\n",
    "    else:\n",
    "        print('All rows are good for ', colName)\n",
    "print(errorMessage)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "47a87f85-7837-4181-b945-4e99e1f099f4",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": []
     },
     "metadata": {
      "application/vnd.databricks.v1+output": {
       "arguments": {},
       "data": "",
       "errorSummary": "Command skipped",
       "errorTraceType": "ansi",
       "metadata": {},
       "type": "ipynbError"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "if errorFlag:\n",
    "    dbutils.fs.mv('/mnt/input/'+fileName,'/mnt/rejected/'+fileName )\n",
    "    dbutils.notebook.exit('{\"errorFlag\": \"true\", \"errorMessage\":\"'+errorMessage +'\"}')\n",
    "else:\n",
    "    dbutils.fs.mv('/mnt/input/'+fileName,'/mnt/staging/'+fileName )\n",
    "    dbutils.notebook.exit('{\"errorFlag\": \"false\", \"errorMessage\":\"No error\"}')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "Project2",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
